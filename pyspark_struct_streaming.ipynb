{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('ProductionPlannerCaller': pipenv)",
   "display_name": "Python 3.8.6 64-bit ('ProductionPlannerCaller': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "d35c8b516a90ec349fb27354d3f5b0fd5f2200bb14aca3e9a64f3cb0fb9d61f2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x18c140ba7c0>"
      ],
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.128.130.64:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>StructStreaming</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('StructStreaming').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "source": [
    "### A full loop reading data from kafka"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "spark.readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"subscribe\", \"input\")\n",
    "  .load()  ### reading IN\n",
    "  .groupby('value.case(\"string\") as key')\n",
    "  .agg(count(\"*\") as \"value\") ### transforming\n",
    "  .writeStream()\n",
    "  .format(\"kafka\")\n",
    "  .option(\"topic\", \"output\") ## writing OUT\n",
    "  .trigger(\"1 minute\")\n",
    "  .outputMode(\"update\")  ## trigger\n",
    "  .option(\"checkpointLocation\", \"...\") \n",
    "  .withWaterMark(\"timestamp\" \"2 minutes\") ## check point\n",
    "  .start()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "# From raw data to bronze table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### read stream \"text\" format as STRING data type, load from \"streamingPath\", then create a tempView\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream\n",
    "  .format(\"text\")\n",
    "  .schema(\"data STRING\")\n",
    "  .option(\"maxFilesPerTrigger\", 1)  # This is used for testing to simulate 1 file arriving at a time.  Generally, don't set this in production.\n",
    "  .load(streamingPath)\n",
    "  .createOrReplaceTempView(\"recordings_raw_temp\") # The path can be here or set as another option just as the kafka example"
   ]
  },
  {
   "source": [
    "### Some transformations on the recordings_raw_temp, creating a tempView"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW recordings_bronze_temp AS (\n",
    "  SELECT current_timestamp() receipt_time, \"recordings\" dataset, *\n",
    "  FROM recordings_raw_temp\n",
    ")"
   ]
  },
  {
   "source": [
    "### write out the transformed tempView"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"recordings_bronze_temp\")\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", bronzeCheckpoint)\n",
    "  .outputMode(\"append\")\n",
    "  .start(bronzePath)  ## saving to brozePath in delta format"
   ]
  },
  {
   "source": [
    "# From bronze to silver"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### load data from bronze table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream\n",
    "  .format(\"delta\")\n",
    "  .load(bronzePath) # from where\n",
    "  .createOrReplaceTempView(\"bronze_unparsed_temp\")"
   ]
  },
  {
   "source": [
    "### data transforming in SQL\n",
    "Parse JSON data from source of \"recordings\"\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW recordings_parsed_temp AS\n",
    "  SELECT json.device_id device_id, json.mrn mrn, json.heartrate heartrate, json.time time \n",
    "  FROM (\n",
    "    SELECT from_json(data, \"device_id INTEGER, mrn LONG, heartrate DOUBLE, time DOUBLE\") json\n",
    "    FROM bronze_unparsed_temp\n",
    "    WHERE dataset = \"recordings\")"
   ]
  },
  {
   "source": [
    "### Write the parsed JSON data to silver_table_1(recordings_parsed_temp)\n",
    "in delta format, also configure a checkpoint path"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "spark.table(\"recordings_parsed_temp\")\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"append\")\n",
    "  .option(\"checkpointLocation\", recordingsParsedCheckpoint)\n",
    "  .start(recordingsParsedPath) # to where"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Load another table for enrichment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark\n",
    "  .read\n",
    "  .format(\"csv\")\n",
    "  .schema(\"mrn STRING, name STRING\")\n",
    "  .option(\"header\", True)\n",
    "  .load(f\"{source_dir}/patient/patient_info.csv\")\n",
    "  .createOrReplaceTempView(\"pii\")\n",
    "\n",
    "# Or use SQL"
   ]
  },
  {
   "source": [
    "### Load parsed data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream\n",
    "  .format(\"delta\")\n",
    "  .load(recordingsParsedPath)\n",
    "  .createOrReplaceTempView(\"silver_recordings_temp\")"
   ]
  },
  {
   "source": [
    "## Enrich Join"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW recordings_w_pii AS (\n",
    "  SELECT device_id, a.mrn, b.name, cast(from_unixtime(time, 'yyyy-MM-dd HH:mm:ss') AS timestamp) time, heartrate\n",
    "  FROM silver_recordings_temp a\n",
    "  INNER JOIN pii b\n",
    "  ON a.mrn = b.mrn\n",
    "  WHERE heartrate > 0)"
   ]
  },
  {
   "source": [
    "### Write out the Enriched data to silver_table_2(recordings_w_pii)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"recordings_w_pii\")\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", recordingsEnrichedCheckpoint)\n",
    "  .outputMode(\"append\")\n",
    "  .start(recordingsEnrichedPath)"
   ]
  },
  {
   "source": [
    "# From Silver to Gold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Read in silver_table_2 from recordingsEnrichedPath"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream\n",
    "  .format(\"delta\")\n",
    "  .load(recordingsEnrichedPath)\n",
    "  .createOrReplaceTempView(\"recordings_enriched_temp\")"
   ]
  },
  {
   "source": [
    "### Transform data with aggregation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW patient_avg AS (\n",
    "  SELECT mrn, name, MEAN(heartrate) avg_heartrate, date_trunc(\"DD\", time) date\n",
    "  FROM recordings_enriched_temp\n",
    "  GROUP BY mrn, name, date_trunc(\"DD\", time))"
   ]
  },
  {
   "source": [
    "### write the aggregation table to dailyAvgPath"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"patient_avg\")\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"complete\")\n",
    "  .option(\"checkpointLocation\", dailyAvgCheckpoint)\n",
    "  .trigger(once=True) # only run once then stop this stream\n",
    "  .start(dailyAvgPath)"
   ]
  },
  {
   "source": [
    "Note that we're using `.trigger(once=True)` above. This provides us the ability to continue to use the strengths of structured streaming while trigger this job as a single batch. To recap, these strengths include:\n",
    "- exactly once end-to-end fault tolerant processing\n",
    "- automatic detection of changes in upstream data sources\n",
    "\n",
    "### In other words, if every pipeline showing above has configured .trigger(once = True), then this Notebook job can be triggered by scheduler once a day/hours (for batch processing)\n",
    "### one use case can be streaming collecting data, batch processing data etc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Register gold_table (patient_avg) to Hive Metastore"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  DROP TABLE IF EXISTS daily_patient_avg\n",
    "\"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE daily_patient_avg\n",
    "  USING DELTA\n",
    "  LOCATION '{dailyAvgPath}'\n",
    "\"\"\")"
   ]
  },
  {
   "source": [
    "# Now can query the saved Gold Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}