{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1604541863658",
   "display_name": "Python 3.8.6 64-bit ('PlanningClient': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using spark context :\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# conf = SparkConf().setAppName('temp')\n",
    "# sc = SparkContext(conf=conf)\n",
    "# print(sc)\n",
    "# data = [1, 2, 3, 4, 5]\n",
    "# distData = sc.parallelize(data)\n",
    "# distData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spark sessiion :\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x23173743c10>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.128.130.98:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>temp</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"temp\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+-----+------+\n| id| dept|salary|\n+---+-----+------+\n|  1|sales|  4200|\n|  2|admin|  3100|\n|  3|sales|  4000|\n|  4|sales|  4000|\n|  5|admin|  2700|\n|  6|  dev|  3400|\n|  7|  dev|  5200|\n|  8|  dev|  3700|\n|  9|  dev|  4400|\n| 10|  dev|  4400|\n+---+-----+------+\n\n"
    }
   ],
   "source": [
    "l = [\n",
    "    (1, \"sales\", 4200),\n",
    "    (2, \"admin\", 3100),\n",
    "    (3, \"sales\", 4000),\n",
    "    (4, \"sales\", 4000),\n",
    "    (5, \"admin\", 2700),\n",
    "    (6, \"dev\", 3400),\n",
    "    (7, \"dev\", 5200),\n",
    "    (8, \"dev\", 3700),\n",
    "    (9, \"dev\", 4400),\n",
    "    (10, \"dev\", 4400)    \n",
    "]\n",
    "df = spark.createDataFrame(l, schema=['id', 'dept', 'salary'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'sales'"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# df.collect() is returning a list\n",
    "df.collect()[3]['dept']\n",
    "# or \n",
    "# df.collect()[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df.groupBy(\"dept\").agg(F.expr('avg(salary)'), \n",
    "                                  F.expr('sum(salary)'),\n",
    "                                  F.expr('collect_list(salary)').alias(\"list of salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+------------------+-----------+--------------------+\n| dept|       avg(salary)|sum(salary)|      list of salary|\n+-----+------------------+-----------+--------------------+\n|  dev|            4220.0|      21100|[3400, 5200, 3700...|\n|sales|4066.6666666666665|      12200|  [4200, 4000, 4000]|\n|admin|            2900.0|       5800|        [3100, 2700]|\n+-----+------------------+-----------+--------------------+\n\n"
    }
   ],
   "source": [
    "df_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on function partitionBy in module pyspark.sql.window:\n\npartitionBy(*cols)\n    Creates a :class:`WindowSpec` with the partitioning defined.\n    \n    .. versionadded:: 1.4\n\n"
    }
   ],
   "source": [
    "help(Window.partitionBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+-----+------+----------+\n| id| dept|salary|max_salary|\n+---+-----+------+----------+\n|  6|  dev|  3400|      5200|\n|  7|  dev|  5200|      5200|\n|  8|  dev|  3700|      5200|\n|  9|  dev|  4400|      5200|\n| 10|  dev|  4400|      5200|\n|  1|sales|  4200|      4200|\n|  3|sales|  4000|      4200|\n|  4|sales|  4000|      4200|\n|  2|admin|  3100|      3100|\n|  5|admin|  2700|      3100|\n+---+-----+------+----------+\n\n"
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy('dept') # specify the partition window\n",
    "df.withColumn(\"max_salary\", F.max(F.col(\"salary\")).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+-----+------+------------------+-----------+--------------------+\n| id| dept|salary|       avg(salary)|sum(salary)|      list of salary|\n+---+-----+------+------------------+-----------+--------------------+\n|  6|  dev|  3400|            4220.0|      21100|[3400, 5200, 3700...|\n|  7|  dev|  5200|            4220.0|      21100|[3400, 5200, 3700...|\n|  8|  dev|  3700|            4220.0|      21100|[3400, 5200, 3700...|\n|  9|  dev|  4400|            4220.0|      21100|[3400, 5200, 3700...|\n| 10|  dev|  4400|            4220.0|      21100|[3400, 5200, 3700...|\n|  1|sales|  4200|4066.6666666666665|      12200|  [4200, 4000, 4000]|\n|  3|sales|  4000|4066.6666666666665|      12200|  [4200, 4000, 4000]|\n|  4|sales|  4000|4066.6666666666665|      12200|  [4200, 4000, 4000]|\n|  2|admin|  3100|            2900.0|       5800|        [3100, 2700]|\n|  5|admin|  2700|            2900.0|       5800|        [3100, 2700]|\n+---+-----+------+------------------+-----------+--------------------+\n\n"
    }
   ],
   "source": [
    "# partition by returns every row calculating over the window\n",
    "df.withColumn(\"avg(salary)\", F.avg(F.col(\"salary\")).over(windowSpec)) \\\n",
    "  .withColumn(\"sum(salary)\", F.sum(F.col(\"salary\")).over(windowSpec)) \\\n",
    "  .withColumn(\"list of salary\", F.collect_list(\"salary\").over(windowSpec)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+-----+------+------------------+-----------+--------------------+\n| id| dept|salary|       avg(salary)|sum(salary)|      list of salary|\n+---+-----+------+------------------+-----------+--------------------+\n|  6|  dev|  3400|            3400.0|       3400|              [3400]|\n|  8|  dev|  3700|            3550.0|       7100|        [3400, 3700]|\n|  9|  dev|  4400|            3975.0|      15900|[3400, 3700, 4400...|\n| 10|  dev|  4400|            3975.0|      15900|[3400, 3700, 4400...|\n|  7|  dev|  5200|            4220.0|      21100|[3400, 3700, 4400...|\n|  3|sales|  4000|            4000.0|       8000|        [4000, 4000]|\n|  4|sales|  4000|            4000.0|       8000|        [4000, 4000]|\n|  1|sales|  4200|4066.6666666666665|      12200|  [4000, 4000, 4200]|\n|  5|admin|  2700|            2700.0|       2700|              [2700]|\n|  2|admin|  3100|            2900.0|       5800|        [2700, 3100]|\n+---+-----+------+------------------+-----------+--------------------+\n\n"
    }
   ],
   "source": [
    "windowSpec_new = Window.partitionBy('dept').orderBy(F.asc('salary')) # new partition window\n",
    "df.withColumn(\"avg(salary)\", F.avg(F.col(\"salary\")).over(windowSpec_new)) \\\n",
    "  .withColumn(\"sum(salary)\", F.sum(F.col(\"salary\")).over(windowSpec_new)) \\\n",
    "  .withColumn(\"list of salary\", F.collect_list(\"salary\").over(windowSpec_new)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on WindowSpec in module pyspark.sql.window object:\n\nclass WindowSpec(builtins.object)\n |  WindowSpec(jspec)\n |  \n |  A window specification that defines the partitioning, ordering,\n |  and frame boundaries.\n |  \n |  Use the static methods in :class:`Window` to create a :class:`WindowSpec`.\n |  \n |  .. versionadded:: 1.4\n |  \n |  Methods defined here:\n |  \n |  __init__(self, jspec)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  orderBy(self, *cols)\n |      Defines the ordering columns in a :class:`WindowSpec`.\n |      \n |      :param cols: names of columns or expressions\n |      \n |      .. versionadded:: 1.4\n |  \n |  partitionBy(self, *cols)\n |      Defines the partitioning columns in a :class:`WindowSpec`.\n |      \n |      :param cols: names of columns or expressions\n |      \n |      .. versionadded:: 1.4\n |  \n |  rangeBetween(self, start, end)\n |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n |      \n |      Both `start` and `end` are relative from the current row. For example,\n |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n |      and \"5\" means the five off after the current row.\n |      \n |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n |      values directly.\n |      \n |      :param start: boundary start, inclusive.\n |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n |                    any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n |      :param end: boundary end, inclusive.\n |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n |                  any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n |      \n |      .. versionadded:: 1.4\n |  \n |  rowsBetween(self, start, end)\n |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n |      \n |      Both `start` and `end` are relative positions from the current row.\n |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n |      the current row, and \"5\" means the fifth row after the current row.\n |      \n |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n |      values directly.\n |      \n |      :param start: boundary start, inclusive.\n |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n |                    any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n |      :param end: boundary end, inclusive.\n |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n |                  any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n |      \n |      .. versionadded:: 1.4\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
    }
   ],
   "source": [
    "help(windowSpec_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without taking duplicates into count\n",
    "windowSpec_new_1 = (\n",
    "    Window.partitionBy('dept')\n",
    "          .orderBy(F.asc('salary'))\n",
    "          .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+-----+------+------------------+-----------+--------------------+\n| id| dept|salary|       avg(salary)|sum(salary)|      list of salary|\n+---+-----+------+------------------+-----------+--------------------+\n|  6|  dev|  3400|            3400.0|       3400|              [3400]|\n|  8|  dev|  3700|            3550.0|       7100|        [3400, 3700]|\n|  9|  dev|  4400|3833.3333333333335|      11500|  [3400, 3700, 4400]|\n| 10|  dev|  4400|            3975.0|      15900|[3400, 3700, 4400...|\n|  7|  dev|  5200|            4220.0|      21100|[3400, 3700, 4400...|\n|  3|sales|  4000|            4000.0|       4000|              [4000]|\n|  4|sales|  4000|            4000.0|       8000|        [4000, 4000]|\n|  1|sales|  4200|4066.6666666666665|      12200|  [4000, 4000, 4200]|\n|  5|admin|  2700|            2700.0|       2700|              [2700]|\n|  2|admin|  3100|            2900.0|       5800|        [2700, 3100]|\n+---+-----+------+------------------+-----------+--------------------+\n\n"
    }
   ],
   "source": [
    "df.withColumn(\"avg(salary)\", F.avg(F.col(\"salary\")).over(windowSpec_new_1)) \\\n",
    "  .withColumn(\"sum(salary)\", F.sum(F.col(\"salary\")).over(windowSpec_new_1)) \\\n",
    "  .withColumn(\"list of salary\", F.collect_list(\"salary\").over(windowSpec_new_1)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Column<b'avg(salary) OVER (PARTITION BY dept ORDER BY salary ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)'>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "F.avg(F.col(\"salary\")).over(windowSpec_new_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Column<b'avg(salary) OVER (PARTITION BY dept ORDER BY salary ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND CURRENT ROW)'>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# without taking duplicates into count\n",
    "windowSpec_new_2 = (\n",
    "    Window.partitionBy('dept')\n",
    "          .orderBy(F.asc('salary'))\n",
    "          .rowsBetween(-1, Window.currentRow) # (start, end)\n",
    ")\n",
    "F.avg(F.col(\"salary\")).over(windowSpec_new_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+-----+------+-----------+-----------+--------------+\n| id| dept|salary|avg(salary)|sum(salary)|list of salary|\n+---+-----+------+-----------+-----------+--------------+\n|  6|  dev|  3400|     3400.0|       3400|        [3400]|\n|  8|  dev|  3700|     3550.0|       7100|  [3400, 3700]|\n|  9|  dev|  4400|     4050.0|       8100|  [3700, 4400]|\n| 10|  dev|  4400|     4400.0|       8800|  [4400, 4400]|\n|  7|  dev|  5200|     4800.0|       9600|  [4400, 5200]|\n|  3|sales|  4000|     4000.0|       4000|        [4000]|\n|  4|sales|  4000|     4000.0|       8000|  [4000, 4000]|\n|  1|sales|  4200|     4100.0|       8200|  [4000, 4200]|\n|  5|admin|  2700|     2700.0|       2700|        [2700]|\n|  2|admin|  3100|     2900.0|       5800|  [2700, 3100]|\n+---+-----+------+-----------+-----------+--------------+\n\n"
    }
   ],
   "source": [
    "df.withColumn(\"avg(salary)\", F.avg(F.col(\"salary\")).over(windowSpec_new_2)) \\\n",
    "  .withColumn(\"sum(salary)\", F.sum(F.col(\"salary\")).over(windowSpec_new_2)) \\\n",
    "  .withColumn(\"list of salary\", F.collect_list(\"salary\").over(windowSpec_new_2)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+-----+------+------------------+------------+----+----------+---------+\n| id| dept|salary|    average_salary|total_salary|rank|dense_rank|perc_rank|\n+---+-----+------+------------------+------------+----+----------+---------+\n|  6|  dev|  3400|            3400.0|        3400|   1|         1|      0.0|\n|  8|  dev|  3700|            3550.0|        7100|   2|         2|     0.25|\n|  9|  dev|  4400|            3975.0|       15900|   3|         3|      0.5|\n| 10|  dev|  4400|            3975.0|       15900|   3|         3|      0.5|\n|  7|  dev|  5200|            4220.0|       21100|   5|         4|      1.0|\n|  3|sales|  4000|            4000.0|        8000|   1|         1|      0.0|\n|  4|sales|  4000|            4000.0|        8000|   1|         1|      0.0|\n|  1|sales|  4200|4066.6666666666665|       12200|   3|         2|      1.0|\n|  5|admin|  2700|            2700.0|        2700|   1|         1|      0.0|\n|  2|admin|  3100|            2900.0|        5800|   2|         2|      1.0|\n+---+-----+------+------------------+------------+----+----------+---------+\n\n"
    }
   ],
   "source": [
    "df_new =  df.withColumn(\"average_salary\", F.avg(F.col('salary')).over(windowSpec_new)) \\\n",
    "            .withColumn(\"total_salary\", F.sum(F.col('salary')).over(windowSpec_new)) \\\n",
    "            .withColumn(\"rank\", F.rank().over(windowSpec_new)) \\\n",
    "            .withColumn(\"dense_rank\", F.dense_rank().over(windowSpec_new)) \\\n",
    "            .withColumn(\"perc_rank\", F.percent_rank().over(windowSpec_new)) \n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on method pivot in module pyspark.sql.group:\n\npivot(pivot_col, values=None) method of pyspark.sql.group.GroupedData instance\n    Pivots a column of the current :class:`DataFrame` and perform the specified aggregation.\n    There are two versions of pivot function: one that requires the caller to specify the list\n    of distinct values to pivot on, and one that does not. The latter is more concise but less\n    efficient, because Spark needs to first compute the list of distinct values internally.\n    \n    :param pivot_col: Name of the column to pivot.\n    :param values: List of values that will be translated to columns in the output DataFrame.\n    \n    # Compute the sum of earnings for each year by course with each course as a separate column\n    \n    >>> df4.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").collect()\n    [Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]\n    \n    # Or without specifying column values (less efficient)\n    \n    >>> df4.groupBy(\"year\").pivot(\"course\").sum(\"earnings\").collect()\n    [Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]\n    >>> df5.groupBy(\"sales.year\").pivot(\"sales.course\").sum(\"sales.earnings\").collect()\n    [Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]\n    \n    .. versionadded:: 1.6\n\n"
    }
   ],
   "source": [
    "# pivot and unpivot\n",
    "help(df.groupBy().pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+-----+----+-----+\n|rank|admin| dev|sales|\n+----+-----+----+-----+\n|   1| 2700|3400| 8000|\n|   2| 3100|3700| null|\n|   3| null|8800| 4200|\n|   5| null|5200| null|\n+----+-----+----+-----+\n\n"
    }
   ],
   "source": [
    "df_new.groupBy(\"rank\").pivot(\"dept\").sum(\"salary\").orderBy(F.asc('rank')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+----------------+-------------------------+--------------+-----------------------+----------------+-------------------------+\n|rank|admin_sum_salary|admin_avg(average_salary)|dev_sum_salary|dev_avg(average_salary)|sales_sum_salary|sales_avg(average_salary)|\n+----+----------------+-------------------------+--------------+-----------------------+----------------+-------------------------+\n|   1|            2700|                   2700.0|          3400|                 3400.0|            8000|                   4000.0|\n|   3|            null|                     null|          8800|                 3975.0|            4200|       4066.6666666666665|\n|   5|            null|                     null|          5200|                 4220.0|            null|                     null|\n|   2|            3100|                   2900.0|          3700|                 3550.0|            null|                     null|\n+----+----------------+-------------------------+--------------+-----------------------+----------------+-------------------------+\n\n"
    }
   ],
   "source": [
    "df_new.groupBy(\"rank\").pivot(\"dept\").agg(\n",
    "    F.sum(\"salary\").alias(\"sum_salary\"),\n",
    "    F.avg(\"average_salary\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+---+---+\n|  A|  B|  C|\n+---+---+---+\n|  G|  X|  1|\n|  G|  Y|  2|\n|  G|  X|  3|\n|  H|  Y|  4|\n|  H|  Z|  5|\n+---+---+---+\n\n"
    }
   ],
   "source": [
    "# another example\n",
    "df_exp = spark.createDataFrame([(\"G\",\"X\", 1),\n",
    "                                (\"G\",\"Y\", 2),\n",
    "                                (\"G\",\"X\", 3),\n",
    "                                (\"H\",\"Y\", 4),\n",
    "                                (\"H\",\"Z\", 5),\n",
    "                                ],list(\"ABC\"))\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+----+---+----+\n|  A|   X|  Y|   Z|\n+---+----+---+----+\n|  G|   4|  2|null|\n|  H|null|  4|   5|\n+---+----+---+----+\n\n"
    }
   ],
   "source": [
    "# group by a column (A), transpose another column to row name \n",
    "df_exp_pivot = df_exp.groupBy(\"A\").pivot(\"B\").sum(\"C\")\n",
    "df_exp_pivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+---+---+\n|  A|  B|  C|\n+---+---+---+\n|  G|  X|  4|\n|  G|  Y|  2|\n|  H|  Y|  4|\n|  H|  Z|  5|\n+---+---+---+\n\n"
    }
   ],
   "source": [
    "df_exp_unpivot = df_exp_pivot.selectExpr(\"A\", \"stack(3, 'X', X, 'Y', Y, 'Z', Z) as (B, C)\").where(\"C is not null\")\n",
    "df_exp_unpivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "++\n||\n++\n||\n||\n||\n||\n++\n\n"
    }
   ],
   "source": [
    "df_exp_unpivot.selectExpr().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on method selectExpr in module pyspark.sql.dataframe:\n\nselectExpr(*expr) method of pyspark.sql.dataframe.DataFrame instance\n    Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n    \n    This is a variant of :func:`select` that accepts SQL expressions.\n    \n    >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n    [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n    \n    .. versionadded:: 1.3\n\n"
    }
   ],
   "source": [
    "help(df.selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+----+\n|col0|col1|\n+----+----+\n|   1|   2|\n|   3|   4|\n|   1|   2|\n|   3|   4|\n|   1|   2|\n|   3|   4|\n|   1|   2|\n|   3|   4|\n|   1|   2|\n|   3|   4|\n+----+----+\n\n"
    }
   ],
   "source": [
    "df_exp.selectExpr(\"stack(2, 1, 2, 3, 4)\").show()"
   ]
  }
 ]
}