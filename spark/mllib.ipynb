{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1604969531157",
   "display_name": "Python 3.8.6 64-bit ('ProductionPlannerCaller': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x243238d8910>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.128.130.98:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>ML lib</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ML lib\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from os.path import isfile, join\n",
    "\n",
    "loc = os.path.abspath(\"\")\n",
    "data_path = join(loc, 'ignoreFiles','Synthetic Financial Datasets.csv')\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- step: integer (nullable = true)\n |-- type: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- nameOrig: string (nullable = true)\n |-- oldbalanceOrg: double (nullable = true)\n |-- newbalanceOrig: double (nullable = true)\n |-- nameDest: string (nullable = true)\n |-- oldbalanceDest: double (nullable = true)\n |-- newbalanceDest: double (nullable = true)\n |-- isFraud: integer (nullable = true)\n |-- isFlaggedFraud: integer (nullable = true)\n\n"
    }
   ],
   "source": [
    "# Dataset https://www.kaggle.com/ntnu-testimon/paysim1/data\n",
    "df = spark.read.csv(data_path, inferSchema=True, header= True).sample(withReplacement=True, fraction=0.1, seed=0)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n|step|    type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n|   1| PAYMENT|  3099.97| C249177573|      20771.0|      17671.03|M2096539129|           0.0|           0.0|      0|             0|\n|   1| PAYMENT|  4098.78|C1026483832|     503264.0|     499165.22|M1635378213|           0.0|           0.0|      0|             0|\n|   1| PAYMENT|  1157.86|C1237762639|      21156.0|      19998.14|M1877062907|           0.0|           0.0|      0|             0|\n|   1|TRANSFER| 215310.3|C1670993182|        705.0|           0.0|C1100439041|       22425.0|           0.0|      0|             0|\n|   1| PAYMENT|  6444.64|C1262609629|      12019.0|       5574.36| M587180314|           0.0|           0.0|      0|             0|\n|   1| PAYMENT|  2998.04|  C71802912|      12030.0|       9031.96|M2134271532|           0.0|           0.0|      0|             0|\n|   1|TRANSFER|358831.92| C908084672|          0.0|           0.0| C392292416|     474384.53|    3420103.09|      0|             0|\n|   1| PAYMENT|   661.43| C743648472|      14078.0|      13416.57| M692998280|           0.0|           0.0|      0|             0|\n|   1|TRANSFER|679502.24| C722417467|        290.0|           0.0| C451111351|      171866.0|    3940085.21|      0|             0|\n|   1| PAYMENT|   8550.9|C1795225096|      40060.0|       31509.1| M790094605|           0.0|           0.0|      0|             0|\n+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n\n"
    }
   ],
   "source": [
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------+---------+-------------+--------------+-------+\n|    type|   amount|oldbalanceOrg|newbalanceOrig|isFraud|\n+--------+---------+-------------+--------------+-------+\n| PAYMENT|  3099.97|      20771.0|      17671.03|      0|\n| PAYMENT|  4098.78|     503264.0|     499165.22|      0|\n| PAYMENT|  1157.86|      21156.0|      19998.14|      0|\n|TRANSFER| 215310.3|        705.0|           0.0|      0|\n| PAYMENT|  6444.64|      12019.0|       5574.36|      0|\n| PAYMENT|  2998.04|      12030.0|       9031.96|      0|\n|TRANSFER|358831.92|          0.0|           0.0|      0|\n| PAYMENT|   661.43|      14078.0|      13416.57|      0|\n|TRANSFER|679502.24|        290.0|           0.0|      0|\n| PAYMENT|   8550.9|      40060.0|       31509.1|      0|\n+--------+---------+-------------+--------------+-------+\n\n"
    }
   ],
   "source": [
    "df = df.select('type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'isFraud') # the df using, 'isFraud' is the label\n",
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "train, test = df.randomSplit([0.7,0.3], seed = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "445270\n"
    }
   ],
   "source": [
    "print(train.count()) # cost 1.3 min on my local machine for 6 million data tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('type', 'string'),\n ('amount', 'double'),\n ('oldbalanceOrg', 'double'),\n ('newbalanceOrig', 'double'),\n ('isFraud', 'int')]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['type']"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# category features waiting to encoding\n",
    "col_cat = [x[0] for x in train.dtypes if x[1] == 'string']\n",
    "col_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['amount', 'oldbalanceOrg', 'newbalanceOrig']"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# numeric features\n",
    "col_num = [x[0] for x in train.dtypes if ((x[1] == 'double') & (x[0] != 'isFraud'))]\n",
    "col_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------+-----------+------------------+-------------------+\n|    type|count(type)|avg(oldbalanceOrg)|avg(newbalanceOrig)|\n+--------+-----------+------------------+-------------------+\n|TRANSFER|      37221| 55395.47344402353|  10964.96704333575|\n| CASH_IN|      97944|3624019.4883186365| 3793578.2936142087|\n|CASH_OUT|     156412| 46732.76179890292|  17636.13890845972|\n| PAYMENT|     150753|  69400.7541342461|  62998.74784448732|\n|   DEBIT|       2940|  65146.0245782313|  61939.99417346938|\n+--------+-----------+------------------+-------------------+\n\n"
    }
   ],
   "source": [
    "# OneHot Encoding\n",
    "# train data in a nutshell\n",
    "train.groupBy(F.col('type')).agg(F.count(F.col('type')), \n",
    "                                 F.avg(F.col('oldbalanceOrg')),\n",
    "                                 F.avg(F.col('newbalanceOrig'))\n",
    "                                 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLib import\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on class StringIndexer in module pyspark.ml.feature:\n\nclass StringIndexer(pyspark.ml.wrapper.JavaEstimator, _StringIndexerParams, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n |  StringIndexer(inputCol=None, outputCol=None, inputCols=None, outputCols=None, handleInvalid='error', stringOrderType='frequencyDesc')\n |  \n |  A label indexer that maps a string column of labels to an ML column of label indices.\n |  If the input column is numeric, we cast it to string and index the string values.\n |  The indices are in [0, numLabels). By default, this is ordered by label frequencies\n |  so the most frequent label gets index 0. The ordering behavior is controlled by\n |  setting :py:attr:`stringOrderType`. Its default value is 'frequencyDesc'.\n |  \n |  >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\",\n |  ...     stringOrderType=\"frequencyDesc\")\n |  >>> stringIndexer.setHandleInvalid(\"error\")\n |  StringIndexer...\n |  >>> model = stringIndexer.fit(stringIndDf)\n |  >>> model.setHandleInvalid(\"error\")\n |  StringIndexerModel...\n |  >>> td = model.transform(stringIndDf)\n |  >>> sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n |  ...     key=lambda x: x[0])\n |  [(0, 0.0), (1, 2.0), (2, 1.0), (3, 0.0), (4, 0.0), (5, 1.0)]\n |  >>> inverter = IndexToString(inputCol=\"indexed\", outputCol=\"label2\", labels=model.labels)\n |  >>> itd = inverter.transform(td)\n |  >>> sorted(set([(i[0], str(i[1])) for i in itd.select(itd.id, itd.label2).collect()]),\n |  ...     key=lambda x: x[0])\n |  [(0, 'a'), (1, 'b'), (2, 'c'), (3, 'a'), (4, 'a'), (5, 'c')]\n |  >>> stringIndexerPath = temp_path + \"/string-indexer\"\n |  >>> stringIndexer.save(stringIndexerPath)\n |  >>> loadedIndexer = StringIndexer.load(stringIndexerPath)\n |  >>> loadedIndexer.getHandleInvalid() == stringIndexer.getHandleInvalid()\n |  True\n |  >>> modelPath = temp_path + \"/string-indexer-model\"\n |  >>> model.save(modelPath)\n |  >>> loadedModel = StringIndexerModel.load(modelPath)\n |  >>> loadedModel.labels == model.labels\n |  True\n |  >>> loadedModel.transform(stringIndDf).take(1) == model.transform(stringIndDf).take(1)\n |  True\n |  >>> indexToStringPath = temp_path + \"/index-to-string\"\n |  >>> inverter.save(indexToStringPath)\n |  >>> loadedInverter = IndexToString.load(indexToStringPath)\n |  >>> loadedInverter.getLabels() == inverter.getLabels()\n |  True\n |  >>> stringIndexer.getStringOrderType()\n |  'frequencyDesc'\n |  >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\",\n |  ...     stringOrderType=\"alphabetDesc\")\n |  >>> model = stringIndexer.fit(stringIndDf)\n |  >>> td = model.transform(stringIndDf)\n |  >>> sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n |  ...     key=lambda x: x[0])\n |  [(0, 2.0), (1, 1.0), (2, 0.0), (3, 2.0), (4, 2.0), (5, 0.0)]\n |  >>> fromlabelsModel = StringIndexerModel.from_labels([\"a\", \"b\", \"c\"],\n |  ...     inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\")\n |  >>> result = fromlabelsModel.transform(stringIndDf)\n |  >>> sorted(set([(i[0], i[1]) for i in result.select(result.id, result.indexed).collect()]),\n |  ...     key=lambda x: x[0])\n |  [(0, 0.0), (1, 1.0), (2, 2.0), (3, 0.0), (4, 0.0), (5, 2.0)]\n |  >>> testData = sc.parallelize([Row(id=0, label1=\"a\", label2=\"e\"),\n |  ...                            Row(id=1, label1=\"b\", label2=\"f\"),\n |  ...                            Row(id=2, label1=\"c\", label2=\"e\"),\n |  ...                            Row(id=3, label1=\"a\", label2=\"f\"),\n |  ...                            Row(id=4, label1=\"a\", label2=\"f\"),\n |  ...                            Row(id=5, label1=\"c\", label2=\"f\")], 3)\n |  >>> multiRowDf = spark.createDataFrame(testData)\n |  >>> inputs = [\"label1\", \"label2\"]\n |  >>> outputs = [\"index1\", \"index2\"]\n |  >>> stringIndexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n |  >>> model = stringIndexer.fit(multiRowDf)\n |  >>> result = model.transform(multiRowDf)\n |  >>> sorted(set([(i[0], i[1], i[2]) for i in result.select(result.id, result.index1,\n |  ...     result.index2).collect()]), key=lambda x: x[0])\n |  [(0, 0.0, 1.0), (1, 2.0, 0.0), (2, 1.0, 1.0), (3, 0.0, 0.0), (4, 0.0, 0.0), (5, 1.0, 0.0)]\n |  >>> fromlabelsModel = StringIndexerModel.from_arrays_of_labels([[\"a\", \"b\", \"c\"], [\"e\", \"f\"]],\n |  ...     inputCols=inputs, outputCols=outputs)\n |  >>> result = fromlabelsModel.transform(multiRowDf)\n |  >>> sorted(set([(i[0], i[1], i[2]) for i in result.select(result.id, result.index1,\n |  ...     result.index2).collect()]), key=lambda x: x[0])\n |  [(0, 0.0, 0.0), (1, 1.0, 1.0), (2, 2.0, 0.0), (3, 0.0, 1.0), (4, 0.0, 1.0), (5, 2.0, 1.0)]\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  Method resolution order:\n |      StringIndexer\n |      pyspark.ml.wrapper.JavaEstimator\n |      _StringIndexerParams\n |      pyspark.ml.wrapper.JavaParams\n |      pyspark.ml.wrapper.JavaWrapper\n |      pyspark.ml.base.Estimator\n |      pyspark.ml.param.shared.HasHandleInvalid\n |      pyspark.ml.param.shared.HasInputCol\n |      pyspark.ml.param.shared.HasOutputCol\n |      pyspark.ml.param.shared.HasInputCols\n |      pyspark.ml.param.shared.HasOutputCols\n |      pyspark.ml.param.Params\n |      pyspark.ml.util.Identifiable\n |      pyspark.ml.util.JavaMLReadable\n |      pyspark.ml.util.MLReadable\n |      pyspark.ml.util.JavaMLWritable\n |      pyspark.ml.util.MLWritable\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, inputCol=None, outputCol=None, inputCols=None, outputCols=None, handleInvalid='error', stringOrderType='frequencyDesc')\n |      __init__(self, inputCol=None, outputCol=None, inputCols=None, outputCols=None,                  handleInvalid=\"error\", stringOrderType=\"frequencyDesc\")\n |  \n |  setHandleInvalid(self, value)\n |      Sets the value of :py:attr:`handleInvalid`.\n |  \n |  setInputCol(self, value)\n |      Sets the value of :py:attr:`inputCol`.\n |  \n |  setInputCols(self, value)\n |      Sets the value of :py:attr:`inputCols`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  setOutputCol(self, value)\n |      Sets the value of :py:attr:`outputCol`.\n |  \n |  setOutputCols(self, value)\n |      Sets the value of :py:attr:`outputCols`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  setParams(self, inputCol=None, outputCol=None, inputCols=None, outputCols=None, handleInvalid='error', stringOrderType='frequencyDesc')\n |      setParams(self, inputCol=None, outputCol=None, inputCols=None, outputCols=None,                   handleInvalid=\"error\", stringOrderType=\"frequencyDesc\")\n |      Sets params for this StringIndexer.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setStringOrderType(self, value)\n |      Sets the value of :py:attr:`stringOrderType`.\n |      \n |      .. versionadded:: 2.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n |  \n |  __metaclass__ = <class 'abc.ABCMeta'>\n |      Metaclass for defining Abstract Base Classes (ABCs).\n |      \n |      Use this metaclass to create an ABC.  An ABC can be subclassed\n |      directly, and then acts as a mix-in class.  You can also register\n |      unrelated concrete classes (even built-in classes) and unrelated\n |      ABCs as 'virtual subclasses' -- these and their descendants will\n |      be considered subclasses of the registering ABC by the built-in\n |      issubclass() function, but the registering ABC won't show up in\n |      their MRO (Method Resolution Order) nor will method\n |      implementations defined by the registering ABC be callable (not\n |      even via super()).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from _StringIndexerParams:\n |  \n |  getStringOrderType(self)\n |      Gets the value of :py:attr:`stringOrderType` or its default value 'frequencyDesc'.\n |      \n |      .. versionadded:: 2.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from _StringIndexerParams:\n |  \n |  handleInvalid = Param(parent='undefined', name='handleInvalid', ...spe...\n |  \n |  stringOrderType = Param(parent='undefined', name='stringOrderType'...,...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n |  \n |  clear(self, param)\n |      Clears a param from the param map if it has been explicitly set.\n |  \n |  copy(self, extra=None)\n |      Creates a copy of this instance with the same uid and some\n |      extra params. This implementation first calls Params.copy and\n |      then make a copy of the companion Java pipeline component with\n |      extra params. So both the Python wrapper and the Java pipeline\n |      component get copied.\n |      \n |      :param extra: Extra parameters to copy to the new instance\n |      :return: Copy of this instance\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __del__(self)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.base.Estimator:\n |  \n |  fit(self, dataset, params=None)\n |      Fits a model to the input dataset with optional parameters.\n |      \n |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n |                     param maps is given, this calls fit on each param map and returns a list of\n |                     models.\n |      :returns: fitted model(s)\n |      \n |      .. versionadded:: 1.3.0\n |  \n |  fitMultiple(self, dataset, paramMaps)\n |      Fits a model to the input dataset for each param map in `paramMaps`.\n |      \n |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n |      :param paramMaps: A Sequence of param maps.\n |      :return: A thread safe iterable which contains one model for each param map. Each\n |               call to `next(modelIterator)` will return `(index, model)` where model was fit\n |               using `paramMaps[index]`. `index` values may not be sequential.\n |      \n |      .. versionadded:: 2.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasHandleInvalid:\n |  \n |  getHandleInvalid(self)\n |      Gets the value of handleInvalid or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasInputCol:\n |  \n |  getInputCol(self)\n |      Gets the value of inputCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCol:\n |  \n |  inputCol = Param(parent='undefined', name='inputCol', doc='input colum...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n |  \n |  getOutputCol(self)\n |      Gets the value of outputCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n |  \n |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasInputCols:\n |  \n |  getInputCols(self)\n |      Gets the value of inputCols or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCols:\n |  \n |  inputCols = Param(parent='undefined', name='inputCols', doc='input col...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasOutputCols:\n |  \n |  getOutputCols(self)\n |      Gets the value of outputCols or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCols:\n |  \n |  outputCols = Param(parent='undefined', name='outputCols', doc='output ...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.Params:\n |  \n |  explainParam(self, param)\n |      Explains a single param and returns its name, doc, and optional\n |      default value and user-supplied value in a string.\n |  \n |  explainParams(self)\n |      Returns the documentation of all params with their optionally\n |      default values and user-supplied values.\n |  \n |  extractParamMap(self, extra=None)\n |      Extracts the embedded default param values and user-supplied\n |      values, and then merges them with extra values from input into\n |      a flat param map, where the latter value is used if there exist\n |      conflicts, i.e., with ordering: default param values <\n |      user-supplied values < extra.\n |      \n |      :param extra: extra param values\n |      :return: merged param map\n |  \n |  getOrDefault(self, param)\n |      Gets the value of a param in the user-supplied param map or its\n |      default value. Raises an error if neither is set.\n |  \n |  getParam(self, paramName)\n |      Gets a param by its name.\n |  \n |  hasDefault(self, param)\n |      Checks whether a param has a default value.\n |  \n |  hasParam(self, paramName)\n |      Tests whether this instance contains a param with a given\n |      (string) name.\n |  \n |  isDefined(self, param)\n |      Checks whether a param is explicitly set by user or has\n |      a default value.\n |  \n |  isSet(self, param)\n |      Checks whether a param is explicitly set by user.\n |  \n |  set(self, param, value)\n |      Sets a parameter in the embedded param map.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from pyspark.ml.param.Params:\n |  \n |  params\n |      Returns all params ordered by name. The default implementation\n |      uses :py:func:`dir` to get all attributes of type\n |      :py:class:`Param`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.Identifiable:\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n |  \n |  read() from builtins.type\n |      Returns an MLReader instance for this class.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.MLReadable:\n |  \n |  load(path) from builtins.type\n |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n |  \n |  write(self)\n |      Returns an MLWriter instance for this ML instance.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.MLWritable:\n |  \n |  save(self, path)\n |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n\n"
    }
   ],
   "source": [
    "help(StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a general way to encode(multi features making at once)\n",
    "string_indexer = [\n",
    "    StringIndexer(inputCol=x, outputCol=x+'_StringIndexer', handleInvalid= \"skip\")\n",
    "    for x in col_cat\n",
    "] # comperhension expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[OneHotEncoder_318ab543bb7f]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "on_hot_encoder = [\n",
    "    OneHotEncoder(\n",
    "        inputCols=[f\"{x}_StringIndexer\" for x in col_cat],\n",
    "        outputCols=[f\"{x}_OneHotEncoder\" for x in col_cat]\n",
    "    )\n",
    "]\n",
    "on_hot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector assembler -- spark ML only accept vector inputs -- which mean need another data convert\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['amount', 'oldbalanceOrg', 'newbalanceOrig', 'type_OneHotEncoder']"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "assembler_input = [x for x in col_num]\n",
    "assembler_input += [f\"{x}_OneHotEncoder\" for x in col_cat]\n",
    "assembler_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "VectorAssembler_e60cd0add28f"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=assembler_input, outputCol=\"VectorAssembly_features\"\n",
    ")\n",
    "vector_assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------+-----+-------------+\n|categoryIndex|value|  categoryVec|\n+-------------+-----+-------------+\n|          0.0|    x|(4,[0],[1.0])|\n|          1.0|    y|(4,[1],[1.0])|\n|          2.0|    z|(4,[2],[1.0])|\n|          3.0|    o|(4,[3],[1.0])|\n|          4.0|    p|    (4,[],[])|\n|          2.0|    z|(4,[2],[1.0])|\n+-------------+-----+-------------+\n\n"
    }
   ],
   "source": [
    "# more OneHotEncoder\n",
    "\n",
    "df_example = spark.createDataFrame([\n",
    "    (0.0, 'x'),\n",
    "    (1.0, 'y'),\n",
    "    (2.0, 'z'),\n",
    "    (3.0, 'o'),\n",
    "    (4.0, 'p'),\n",
    "    (2.0, 'z')\n",
    "], [\"categoryIndex\", \"value\"])\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"categoryIndex\"],\n",
    "                        outputCols=[\"categoryVec\"])\n",
    "model = encoder.fit(df_example)\n",
    "encoded = model.transform(df_example)\n",
    "encoded.show() # to interpret refer to RDD data type \"Local vector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark ML pipeline -- similar to sklearn\n",
    "# good practice to organize stages in this way\n",
    "stages = []\n",
    "stages += string_indexer\n",
    "stages += on_hot_encoder\n",
    "stages += [vector_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[StringIndexer_6ff6fb2ba74e,\n OneHotEncoder_318ab543bb7f,\n VectorAssembler_e60cd0add28f]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages(stages)\n",
    "model = pipeline.fit(train) # fit train data to generate a transform model -- string_indexer, one_hot_encoder, vector_assembler all are preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df = model.transform(test) # transform test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------+------+-------------+--------------+-------+------------------+------------------+-----------------------+\n|   type|amount|oldbalanceOrg|newbalanceOrig|isFraud|type_StringIndexer|type_OneHotEncoder|VectorAssembly_features|\n+-------+------+-------------+--------------+-------+------------------+------------------+-----------------------+\n|CASH_IN| 53.28|   6317325.17|    6317378.45|      0|               2.0|     (4,[2],[1.0])|   [53.28,6317325.17...|\n|CASH_IN| 83.13|        120.0|        203.13|      0|               2.0|     (4,[2],[1.0])|   [83.13,120.0,203....|\n|CASH_IN|103.95|     79317.56|      79421.51|      0|               2.0|     (4,[2],[1.0])|   [103.95,79317.56,...|\n|CASH_IN|176.14|1.134638683E7| 1.134656297E7|      0|               2.0|     (4,[2],[1.0])|   [176.14,1.1346386...|\n|CASH_IN|226.37|    6095573.3|    6095799.67|      0|               2.0|     (4,[2],[1.0])|   [226.37,6095573.3...|\n|CASH_IN|251.31|   4997835.18|    4998086.48|      0|               2.0|     (4,[2],[1.0])|   [251.31,4997835.1...|\n|CASH_IN|296.07|   2528583.68|    2528879.76|      0|               2.0|     (4,[2],[1.0])|   [296.07,2528583.6...|\n|CASH_IN|321.15|    307358.48|     307679.63|      0|               2.0|     (4,[2],[1.0])|   [321.15,307358.48...|\n|CASH_IN|370.25|   3359816.04|    3360186.29|      0|               2.0|     (4,[2],[1.0])|   [370.25,3359816.0...|\n|CASH_IN|436.84|   6486843.71|    6487280.55|      0|               2.0|     (4,[2],[1.0])|   [436.84,6486843.7...|\n|CASH_IN|438.95|     441650.0|     442088.95|      0|               2.0|     (4,[2],[1.0])|   [438.95,441650.0,...|\n|CASH_IN|442.15|       8633.0|       9075.15|      0|               2.0|     (4,[2],[1.0])|   [442.15,8633.0,90...|\n|CASH_IN|480.84|        122.0|        602.84|      0|               2.0|     (4,[2],[1.0])|   [480.84,122.0,602...|\n|CASH_IN|613.37|1.739764074E7| 1.739825411E7|      0|               2.0|     (4,[2],[1.0])|   [613.37,1.7397640...|\n|CASH_IN|633.12|       9842.5|      10475.62|      0|               2.0|     (4,[2],[1.0])|   [633.12,9842.5,10...|\n|CASH_IN|691.04|   1890586.96|     1891278.0|      0|               2.0|     (4,[2],[1.0])|   [691.04,1890586.9...|\n|CASH_IN| 704.2|   6786535.83|    6787240.02|      0|               2.0|     (4,[2],[1.0])|   [704.2,6786535.83...|\n|CASH_IN|749.43|  2.6238956E7| 2.623970544E7|      0|               2.0|     (4,[2],[1.0])|   [749.43,2.6238956...|\n|CASH_IN|853.99|      12471.0|      13324.99|      0|               2.0|     (4,[2],[1.0])|   [853.99,12471.0,1...|\n|CASH_IN|872.37|      98265.0|      99137.37|      0|               2.0|     (4,[2],[1.0])|   [872.37,98265.0,9...|\n+-------+------+-------------+--------------+-------+------------------+------------------+-----------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "pp_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pp_df.select(\n",
    "    F.col(\"VectorAssembly_features\").alias(\"features\"),\n",
    "    F.col(\"isFraud\").alias(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------------------------------------------------+-----+\n|features                                            |label|\n+----------------------------------------------------+-----+\n|[53.28,6317325.17,6317378.45,0.0,0.0,1.0,0.0]       |0    |\n|[83.13,120.0,203.13,0.0,0.0,1.0,0.0]                |0    |\n|[103.95,79317.56,79421.51,0.0,0.0,1.0,0.0]          |0    |\n|[176.14,1.134638683E7,1.134656297E7,0.0,0.0,1.0,0.0]|0    |\n|[226.37,6095573.3,6095799.67,0.0,0.0,1.0,0.0]       |0    |\n+----------------------------------------------------+-----+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "data.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 8.75 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression().fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.976321534386822"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "model.summary.areaUnderROC ## how ??? taking into model's original inputs?? No wonder the score is this high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}